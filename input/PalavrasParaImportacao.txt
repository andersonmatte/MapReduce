MapReduce é um modelo de programação que foi projetado para permitir processamento paralelo distribuído de largos conjuntos de dados, convertendo-os em um conjunto de listas ordenadas, reduzindo o tamanho destes conjuntos e gerando listas ordenadas ainda menores. Em resumo, MapReduce foi projetado para usar computação paralela distribuída em Big Data e transformar os dados em pedaços menores.

MapReduce funciona através de 2 operações: mapeamento e redução. No processo de mapeamento (Map), os dados são separados em pares (key-value pairs), transformados e filtrados. Então os dado são distribuídos para os nodes e processados. No processo de redução (Reduce), os dados são agregados em conjuntos de dados (datasets) menores. Os dados resultantes do processo de redução são transformados em um formato padrão de chave-valor (key-value), onde a chave (key) funciona como o identificador do registro e o valor (value) é o dado (conteúdo) identificado pela chave. Os nodes do cluster processam as operações de map e reduce que são definidas pelo usuário. Isso é feito de acordo com 2 passos:

Mapeamento dos dados – os dados de entrada são primeiramente distribuídos em pares key-value e divididos em fragmentos, que são então atribuídos a tarefas de mapeamento. Cada cluster (um grupo de nodes conectados um ao outro e que compartilham tarefas de processamento) recebe um número de tarefas de mapeamento que são distribuídas através dos nodes. Durante o processamento dos pares key-value, pares intermediários de key-value são gerados. Estes pares key-value intermediários são classificados e divididos em um novo conjunto de fragmentos. Esses novos fragmentos serão enviados para o processo de redução.

Redução dos dados – cada operação de redução dos dados tem um fragmento atribuído. O processo de redução simplesmente processa o fragmento e produz um output, que é também um par key-value. Processos de redução são também distribuídos entre os diferentes nodes do cluster. Após o fim do processamento, o output final é gravado no file system.

Em resumo, para tratar dados de alto volume, velocidade e variedade (Big Data), é possível usar processos de mapeamento e redução para classificar os dados em pares key-value e reduzi-los em pares menores através de operações de agregação que combinam múltiplos valores de um dataset, em um único valor.